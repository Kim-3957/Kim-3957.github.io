---
layout: post
title: "무향칼만필터 (1)"  
excerpt: "튜토리얼 1"  
categories: [UKalman]
tags: [UKalman]
comments: true
---
# 무향칼만필터 (1)
이름도 이상하다. 무향이라니.  
향기가 없는꽃은 매력이 없지만, 여기서의 향기라는것은 아마도 필터의 개성이 세지 않다는말일 것이다.  
개성이 세지 않다는것은, 아주아주 일반적으로 근사를 했다는 소리겠고.  
또 우리의 친구 가우시안이 등장할것만 같은 생각이 든다.  

# 선형 칼만 필터와의 차이 
*선형 칼만 필터* 라는 이름만 들어도 무언가 단점을 포함하고 있는 기분이 들지 않는가?  
그래서 선형이 안좋은 이유가 무엇인가 하면, 당연하게도 비선형적인 궤적을 그리는 물체라던지,  
아무튼 추정하는 무언가가 비선형으로 움직일때에, 반응할때에, 응답할때에, 출력될때에, 입력될때에,  
우리는 추정치가, 예측치가, 실제의 우리가 추정하고싶은 값으로부터 멀어진다는 이야기가될 것 같다.  

아주아주 간단한 예를 들어보자.  
일정한 각속도 $$\theta$$ 를 가지고 이동하는 이동 로봇을 생각해보자.  
이 예가 좀 이해가 안된다느 사람은, $$\theta$$ 를 가속도라고 생각하고 생각해보자.  
자, 우리가 선형적으로 CV 그러니까 같은 속도로 계속 움직인다는 모델을 구상했을때에,  
샘플링 속도가 어마어마하게 빨라서 그냥 선형근사를 해도 오차가 적은 경우를 제외하고  
띄엄띄엄 위치값을 출력하는 장비가 있는경우, 처음부터 아주아주 근사하게 예측을 할 필요가있다.  

이때에, 선형적으로 속도를 가지고 위치를 예측하는게 아니라, x y 축에 대해서 속도를 분해한 후에, 
좀전의 $$\theta$$ 값을 가지고 비선형적으로 속도와 세타에 대해서 선형근사를 행한후에 위치를 예측 할 필요가 있다.  

$$x_{k} = x_tk+ v \Delta t \cos (\theta_k +w\Delta t/2)$$
$$y_{k} = y_tk+ v \Delta t \sin (\theta_k +w\Delta t/2)$$
$$\theta_k =\theta_{k-1} +w\Delta t$$

이동하는 로봇의 상태방정식을 위와같이 생각하면 선형근사로는 조금 힘든생각이 든다.  
확장칼만필터에서는 저거를 야코비안 (각 파라메터들로 편미분한 행렬) 가지고 선형근사를 하여서  
우리가 저번 시간에 봤었던 상태모델 $$\Phi_k$$ 를 

$$\Phi_k=\begin{bmatrix} 1 &0& -v \Delta t \sin (\theta_k +w\Delta t/2)\\0 &1 &-v \Delta t \cos (\theta_k +w\Delta t/2)\\0&0&1\end{bmatrix}$$

저번 선형칼만 필터를 설명할때에 이 $$\Phi_k$$ 를 $k$ 의함수로 놓는것에 대해서 시치미를 ...  
무튼 $$\Phi_k$$ 는 이제 선형 모델의 야코비안이며 이것을 이용해서 선형칼만 필터의 나머지의 식을 동일하게 사용하는것이 확장칼만 필터이다.  
순서가 반대이지만, 무향칼만 필터가 더 재미있기 때문에, 확장 칼만 필터는 다음에 다루기로 ...  
  
# 무향 변환 (Uncented Transformation)

근묵자흑이라는 말이있다.  
친구따라 강남 간다는 이야기가 있다.  
처음만난 사람을 알려면, 그 주위 사람을 보면 어떤 환경에서 자라왔는지, 어떤 집안 출신인지, 성품은 어떤지 어느정도 정확하게 유추할 수 있으리라 생각되어진다.  

데이터도 똑같다.  
우리가 알고자 하는 $$x$$ 를 다음과같이 가상의 주위 친구들로 구성해보자.  

$$x_k^{(0)} = \mu = \mathbb{E}(x)$$ 
$$x_k^{(1)} = \mu +\sigma$$
$$x_k^{(2)} = \mu -\sigma$$

원한다면 친구를 늘려도 된다.  
하지만, 이 튜토리얼에서는 주위 가장 친한친구 2명만 본다고 하자.  
물론, 친구들중에 질이 조금 나쁜 친구가 있을수도있고하니 편향되지 않게 가중치 $$W^{(i)}$$ 를 둔다고 하자.  
그러면, 우리는 이 두명의 친구와 우리가 알고싶은 $$x_k$$ 에 대해서 다음과같이 쓸 수 있다.  

$$\mu = \Sigma_{i=0}^{2n} W^{(i)} x^{(i)}_k$$

그리고 친구들의 편차를 다음과 같이 쓸 수있다.  

$$\sigma^2 = \Sigma_{i=0}^{2n} W^{(i)} (x^{(i)}_k - \mu)^2$$

이 작업을 무향 변환이라고 한다.  
원한다면, 친구를 늘여서, 데이터를 더 확보 할 수도 있다.  

# 공분산 행렬의 제곱근 (Square root of covariance matrix) 과 
먼저, 위에서 나온 표준편차 $$\sigma$$ 를  계산하는 방법을 생각해보자.  
그전에, 
어떤 스칼라 숫자의 제급근을 구하기는 쉽다.  
하지만, 
